{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "908b5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp digit_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66b8bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Import relevant libraries\n",
    "import tensorflow as tf\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5d977a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Get the relevant dataset and do the required preprocessing\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train).astype('float32')\n",
    "y_test = tf.keras.utils.to_categorical(y_test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf6efd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Define the convolutional model\n",
    "\n",
    "\n",
    "class cnn(tf.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(cnn, self).__init__()\n",
    "        self.W_cnn1 = tf.Variable(tf.random.normal([3, 3, 1, in_features], stddev=0.1),\n",
    "                                  name=\"w_cnn1\")\n",
    "        self.b_cnn1 = tf.Variable(tf.constant(0.1, shape=[in_features]), name=\"b_cnn2\")\n",
    "        self.W_cnn2 = tf.Variable(tf.random.normal([3, 3, in_features, out_features], stddev=0.1),\n",
    "                                  name=\"w_cnn2\")\n",
    "        self.b_cnn2 = tf.Variable(tf.constant(0.1, shape=[out_features]), name=\"b_cnn2\")\n",
    "        self.w1 = tf.Variable(tf.random.normal([1568, 256], stddev=0.1),\n",
    "                              name=\"w1\")\n",
    "        self.b1 = tf.Variable(tf.random.normal([1, 256], stddev=0.1),\n",
    "                              name=\"b1\")\n",
    "        self.w2 = tf.Variable(tf.random.normal([256, 64], stddev=0.1),\n",
    "                              name=\"w2\")\n",
    "        self.b2 = tf.Variable(tf.random.normal([1, 64], stddev=0.1), name=\"b2\")\n",
    "        self.w3 = tf.Variable(tf.random.normal([64, 10], stddev=0.1),\n",
    "                              name=\"w3\")\n",
    "        self.b3 = tf.Variable(tf.random.normal([1, 10], stddev=0.1), name=\"b3\")\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None,28,28,1], tf.float32)])\n",
    "    def __call__(self, x):\n",
    "        x = tf.nn.conv2d(\n",
    "            x, filters=self.W_cnn1, padding='SAME', strides=[1, 1, 1, 1\n",
    "                                                             ]) + self.b_cnn1\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2), padding=\"VALID\")\n",
    "        x = tf.nn.conv2d(\n",
    "            x, filters=self.W_cnn2, padding='SAME', strides=[1, 1, 1, 1\n",
    "                                                             ]) + self.b_cnn2\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2), padding=\"VALID\")\n",
    "        x = tf.reshape(x, [-1, 1568])\n",
    "        x = tf.matmul(x, self.w1) + self.b1\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.matmul(x, self.w2) + self.b2\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.matmul(x, self.w3) + self.b3\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fc2c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Define the parameters\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = y_train.shape[1]\n",
    "learning_rate = 1e-2\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "batch_size = 256\n",
    "test_batch_size = 6000\n",
    "epochs = 10\n",
    "\n",
    "train_loader = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train, y_train)).shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "test_loader = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_test, y_test)).batch(test_batch_size)\n",
    "\n",
    "\n",
    "#Define the loss function\n",
    "lossfunc = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "\n",
    "def cross_entropy(y_label, y_pred):\n",
    "    return (-tf.reduce_sum(y_label * tf.math.log(y_pred + 1.e-10)))\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,amsgrad=True)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = cnn(16, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e970f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "#Fit the model\n",
    "\n",
    "loss = 0\n",
    "accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for x,y in train_loader:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x)\n",
    "            loss_val = lossfunc(y, y_pred)\n",
    "            grads = tape.gradient(loss_val, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "            loss += loss_val\n",
    "    loss_list.append(loss)\n",
    "    correct_pred = 0\n",
    "    for x,y in test_loader:\n",
    "        y = tf.argmax(y,axis=1).numpy()\n",
    "        y_pred = tf.argmax(tf.nn.softmax(model(x)),axis=1).numpy()\n",
    "        correct_pred += (y == y_pred).sum().item()\n",
    "    accuracy = correct_pred / y_test.shape[0]*100\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248805f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Plot the loss and accuracy\n",
    "plt.plot(accuracy_list, label='Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(loss_list,'r', label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev\n",
    "\n",
    "nbdev.export.nb_export('digit_tensorflow.ipynb', 'digit_tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78056acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
